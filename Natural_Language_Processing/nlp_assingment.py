# -*- coding: utf-8 -*-
"""2NLPassingment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-CRFu8zxSFdzwOCzC4L44HtcDni1Qrqt
"""

import numpy as np
import pandas as pd

from nltk.corpus import PlaintextCorpusReader
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

nltk.download('punkt')
import string
import matplotlib.pyplot as plt
from wordcloud import WordCloud

!unzip '/content/document classification.zip'

corpus_root1 = '/content/bbc-fulltext (document classification)/bbc/business'

filelists = PlaintextCorpusReader(corpus_root1, '.*')

a=filelists.fileids()

wordslist = filelists.words('510.txt')

print(wordslist)

print(a)

businessCorpus=[]

for file in a:
  wordslist = filelists.words(file)
  businessCorpus.append(wordslist)

print(businessCorpus)

concate_list=[j for i in businessCorpus for j in i]

print(concate_list)

Bcorpus=[]
for file1 in concate_list:

  file1= re.sub('[^a-zA-Z]','',file1) #Replacing the punctuation marks into empty charcter using sub function.
  
  file1=file1.lower()  
 
  ps=PorterStemmer()
 
  file1=ps.stem(file1)

  if file1 not in set(stopwords.words('english')):
    Bcorpus.append(file1)

print(Bcorpus)

Bcorpus1=[]
for i in Bcorpus:
  if i!="":
    Bcorpus1.append(i)

print(Bcorpus1)

"""#business corpus"""

wholeString=' '.join(Bcorpus1)

word_cloud = WordCloud(collocations = False, background_color = 'white').generate(wholeString)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

df=pd.DataFrame({'page':Bcorpus1,'class':"Business"})

print(df)



"""#Entertainment"""

corpus_root2 = '/content/bbc-fulltext (document classification)/bbc/entertainment'

filelists = PlaintextCorpusReader(corpus_root2, '.*')

b=filelists.fileids()

print(b)

entertainmentCorpus=[]

for file in b:
  wordslist = filelists.words(file)
  entertainmentCorpus.append(wordslist)

print(entertainmentCorpus)

concate_list2=[j for i in entertainmentCorpus for j in i]

print(concate_list2)

Ecorpus=[]
for file1 in concate_list2:

  file1= re.sub('[^a-zA-Z]','',file1) #Replacing the punctuation marks into empty charcter using sub function.
  
  file1=file1.lower()  
 
  ps=PorterStemmer()
 
  file1=ps.stem(file1)

  if file1 not in set(stopwords.words('english')):
    Ecorpus.append(file1)

print(Ecorpus)

Ecorpus1=[]
for i in Ecorpus:
  if i!="":
    Ecorpus1.append(i)

print(Ecorpus1)

wholeString=' '.join(Ecorpus1)

word_cloud = WordCloud(collocations = False, background_color = 'white').generate(wholeString)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

df2=pd.DataFrame({'page':Ecorpus1,'class':"Entertainment"})

print(df2)

"""#Politcs"""

corpus_root3 = '/content/bbc-fulltext (document classification)/bbc/politics'

filelists = PlaintextCorpusReader(corpus_root3, '.*')

c=filelists.fileids()

print(c)

politicsCorpus=[]

for file in c:
  wordslist = filelists.words(file)
  politicsCorpus.append(wordslist)

print(politicsCorpus)

concate_list3=[j for i in politicsCorpus for j in i]

print(concate_list3)

Pcorpus=[]
for file1 in concate_list3:

  file1= re.sub('[^a-zA-Z]','',file1) #Replacing the punctuation marks into empty charcter using sub function.
  
  file1=file1.lower()  
 
  ps=PorterStemmer()
 
  file1=ps.stem(file1)

  if file1 not in set(stopwords.words('english')):
    Pcorpus.append(file1)

print(Pcorpus)

Pcorpus1=[]
for i in Pcorpus:
  if i!="":
    Pcorpus1.append(i)

print(Pcorpus1)

wholeString=' '.join(Pcorpus1)

word_cloud = WordCloud(collocations = False, background_color = 'white').generate(wholeString)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

df3=pd.DataFrame({'page':Pcorpus1,'class':"Politics"})

print(df3)

"""#Sport"""

corpus_root4 = '/content/bbc-fulltext (document classification)/bbc/sport'

filelists = PlaintextCorpusReader(corpus_root4, '.*')

d=filelists.fileids()

print(d)

sport_corpus=[]

for file1 in d:
  wordslist = filelists.words(file1)
  sport_corpus.append(wordslist)

# print(sport_corpus)

# concate_list4=[j for i in entertainmentCorpus for j in i]

# print(concate_list2)

# Ecorpus=[]
# for file1 in concate_list2:

#   file1= re.sub('[^a-zA-Z]','',file1) #Replacing the punctuation marks into empty charcter using sub function.
  
#   file1=file1.lower()  
 
#   ps=PorterStemmer()
 
#   file1=ps.stem(file1)

  if file1 not in set(stopwords.words('english')):
    Ecorpus.append(file1)

# print(Ecorpus)

# Ecorpus1=[]
# for i in Ecorpus:
#   if i!="":
#     Ecorpus1.append(i)

# print(Ecorpus1)

#  wholeString=' '.join(Ecorpus1)

# word_cloud = WordCloud(collocations = False, background_color = 'white').generate(wholeString)
# plt.imshow(word_cloud, interpolation='bilinear')
# plt.axis("off")
# plt.show()

# df2=pd.DataFrame({'page':Ecorpus,'class':"Entertainment"})

# print(df2)

"""#tech"""



corpus_root5 = '/content/bbc-fulltext (document classification)/bbc/tech'

filelists = PlaintextCorpusReader(corpus_root5, '.*')

e=filelists.fileids()

print(e)

techCorpus=[]

for file in e:
  wordslist = filelists.words(file)
  techCorpus.append(wordslist)

print(techCorpus)

concate_list4=[j for i in techCorpus for j in i]

print(concate_list4)

Tcorpus=[]
for file1 in concate_list4:

  file1= re.sub('[^a-zA-Z]','',file1) #Replacing the punctuation marks into empty charcter using sub function.
  
  file1=file1.lower()  
 
  ps=PorterStemmer()
 
  file1=ps.stem(file1)

  if file1 not in set(stopwords.words('english')):
    Tcorpus.append(file1)

print(Tcorpus)

Tcorpus1=[]
for i in Tcorpus:
  if i!="":
    Tcorpus1.append(i)

print(Tcorpus1)

wholeString=' '.join(Tcorpus1)

word_cloud = WordCloud(collocations = False, background_color = 'white').generate(wholeString)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis("off")
plt.show()

df4=pd.DataFrame({'page':Tcorpus1,'class':"Tech"})

print(df4)

DF= pd.concat([df,df2,df3,df4], ignore_index = True,axis=0)

print(DF)

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(max_features=1500) #max_features will set the maximum ammount features to be considered by doing vectorization.
x=cv.fit_transform(DF['page']).toarray() 
y=DF.iloc[:,-1].values

print(x)
print(y)



